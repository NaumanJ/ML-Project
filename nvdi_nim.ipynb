{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3173c72-3583-4418-bbcc-57073bddead0",
   "metadata": {},
   "source": [
    "# NVDINO NIM\n",
    "\n",
    "The notebook contains steps for handling image uploads to an AWS S3 bucket, using the asset ID from an API response, followed by invoking NVDINO and NVCLIP for inference with a Gradio UI. Here's a brief outline of the key sections:\n",
    "\n",
    "API Setup and Image Upload:\n",
    "\n",
    "Request an asset ID and upload URL from an API.\n",
    "Convert an image to JPEG and upload it to the AWS S3 link.\n",
    "Inference using NVDINO:\n",
    "\n",
    "Use the uploaded image asset for an inference request via the NVDINO model.\n",
    "SKLearn Model Comparisons:\n",
    "\n",
    "Compare performance with SKLearn models using a custom KNeighborsClassifierMilvus classifier.\n",
    "Interactive UI with Gradio:\n",
    "\n",
    "Create a Gradio UI to test few-shot classification with NVDINO or NVCLIP models.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b22f537-399b-46cb-b65a-5c796b674eb6",
   "metadata": {},
   "source": [
    "This workshop has four parts:\n",
    "\n",
    "**1.** Set up API Interaction and Upload Image to AWS S3  \n",
    "**2.** NVDINO Requests   \n",
    "**3.** Prepare Dataset  \n",
    "**4.** Few Shot Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286e258c-ec8b-4829-a24e-da4334e173ec",
   "metadata": {},
   "source": [
    "# 1: Set up API Interaction and Upload Image to AWS S3\n",
    "\n",
    "We begin by obtaining an upload URL and asset ID from the NIM API, which will allow us to upload an image for inference. Then, we proceed to upload the image to the specified S3 bucket using the provided asset URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d33244-a8de-4876-8b7f-0ccae542fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"nvapi-***\" #FIX ME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d8645-4950-41d2-a510-005470ddb20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install dependecies\n",
    "import sys \n",
    "python_exe = sys.executable\n",
    "!{python_exe} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff8111-477a-446a-8b0f-b46701f3446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pymilvus import MilvusClient\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image \n",
    "import requests \n",
    "import io \n",
    "\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool, CustomJS\n",
    "from bokeh.layouts import column\n",
    "from bokeh.io import output_notebook\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np \n",
    "from bokeh.palettes import Dark2_5 as palette\n",
    "import itertools\n",
    "\n",
    "import requests\n",
    "import io\n",
    "from PIL import Image\n",
    "import logging\n",
    "\n",
    "# Set up logging for better traceability\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Step 1: Get Asset ID and Upload URL from NIM API\n",
    "def get_asset_upload_url(api_url, api_key, asset_name=\"input_image\"):\n",
    "    \"\"\"\n",
    "    Sends a request to the API to retrieve an asset upload URL and ID for an image.\n",
    "    \n",
    "    Args:\n",
    "        api_url (str): The API endpoint to request the asset upload URL.\n",
    "        api_key (str): Authorization key for the API.\n",
    "        asset_name (str): Name for the asset (default: 'input_image').\n",
    "    \n",
    "    Returns:\n",
    "        asset_url (str): The URL for uploading the image.\n",
    "        asset_id (str): The asset ID for referencing in further API requests.\n",
    "    \"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    payload = {\"name\": asset_name}\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(api_url, json=payload, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        return data[\"uploadUrl\"], data[\"assetId\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error fetching asset upload URL: {e}\")\n",
    "        raise\n",
    "\n",
    "# Step 2: Upload image to AWS S3\n",
    "def upload_image_to_s3(asset_url, image_path):\n",
    "    \"\"\"\n",
    "    Uploads an image to AWS S3 using the given asset URL.\n",
    "    \n",
    "    Args:\n",
    "        asset_url (str): The URL to upload the image to.\n",
    "        image_path (str): Local path to the image file.\n",
    "    \"\"\"\n",
    "    s3_headers = {\n",
    "        \"x-amz-meta-nvcf-asset-description\": \"input image\",\n",
    "        \"content-type\": \"image/jpeg\",\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Load and convert image to JPEG\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        buf = io.BytesIO()\n",
    "        image.save(buf, format=\"JPEG\")\n",
    "        \n",
    "        # Upload the image\n",
    "        response = requests.put(asset_url, data=buf.getvalue(), headers=s3_headers, timeout=300)\n",
    "        response.raise_for_status()\n",
    "        logging.info(\"Image successfully uploaded to S3.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error uploading image to S3: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage:\n",
    "api_url = \"https://api.example.com/get-upload-url\"  # Replace with actual API URL\n",
    "api_key = \"your_api_key\"  # Replace with actual API key\n",
    "image_path = \"path_to_image.jpg\"  # Replace with the path to your image\n",
    "\n",
    "try:\n",
    "    asset_url, asset_id = get_asset_upload_url(api_url, api_key)\n",
    "    upload_image_to_s3(asset_url, image_path)\n",
    "    logging.info(f\"Asset ID: {asset_id}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Operation failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e3f956-4325-40d9-ac89-ecef638cec35",
   "metadata": {},
   "source": [
    "Ensure that no errors occured during the installation and import in the two cells above before continuing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c487943-aebf-4ad6-8679-17065f9fe2b6",
   "metadata": {},
   "source": [
    "# 2: Perform Inference using NVDINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e24ce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step: Perform inference with NVDINOv2\n",
    "def perform_inference_nvdino(api_url, api_key, asset_id):\n",
    "    \"\"\"\n",
    "    Sends a request to the NVDINOv2 model for image classification.\n",
    "    \n",
    "    Args:\n",
    "        api_url (str): The API endpoint for inference.\n",
    "        api_key (str): Authorization key for the API.\n",
    "        asset_id (str): The asset ID of the uploaded image.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The inference results.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"NVCF-INPUT-ASSET-REFERENCES\": asset_id,\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    payload = {\"messages\": []}  # Empty payload as the image is referenced in headers\n",
    "\n",
    "    try:\n",
    "        response = requests.post(api_url, json=payload, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Inference error: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage:\n",
    "nvdino_inference_url = \"https://api.example.com/inference\"  # Replace with actual inference API\n",
    "inference_result = perform_inference_nvdino(nvdino_inference_url, api_key, asset_id)\n",
    "logging.info(f\"Inference result: {inference_result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d95eb0-9e5e-46d4-b1bf-7e12fd2e3ef3",
   "metadata": {},
   "source": [
    "To generate an image embedding with NVDINOv2, the first step is to upload the image through the NVCF large asset API, then make the NVDINOv2 NIM API call. \n",
    "\n",
    "In the header, the API key should be presented as a Bearer token and the request body is JSON format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429f87d-7b57-4967-ac7f-d9702639e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#URLs and key \n",
    "assets_url = \"https://api.nvcf.nvidia.com/v2/nvcf/assets\" #large asset upload\n",
    "nvdinov2_url = \"https://ai.api.nvidia.com/v1/stg/cv/nvidia/nv-dinov2\" #nvdinov2 endpoint \n",
    "header_auth = f\"Bearer {api_key}\" #authentication to include in headers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec646d3a-ddbf-4479-8489-346c03c6972c",
   "metadata": {},
   "source": [
    "The first step is to create an asset ID and get an upload link for the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2e4410-7566-4c95-9b57-d7443c747e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1) Send request to get an upload link \n",
    "assets_url = \"https://api.nvcf.nvidia.com/v2/nvcf/assets\"\n",
    "headers = {\n",
    "    \"Authorization\": header_auth,\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"accept\": \"application/json\",\n",
    "}\n",
    "payload = {\"contentType\": f\"image/jpeg\", \"description\": \"input image\"}\n",
    "response = requests.post(assets_url, headers=headers, json=payload, timeout=30)\n",
    "response.raise_for_status()\n",
    "\n",
    "#asset_url is the upload link and asset_id is a unique identifier to reference the image\n",
    "asset_url = response.json()[\"uploadUrl\"]\n",
    "asset_id = response.json()[\"assetId\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590baaee-c219-4c1b-b286-47e05b8c22b8",
   "metadata": {},
   "source": [
    "We now have an asset ID and an AWS S3 link to upload the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020537c7-dfbf-43a9-9296-b8ef50130cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2) Upload image to asset_url \n",
    "s3_headers = {\n",
    "    \"x-amz-meta-nvcf-asset-description\": \"input image\",\n",
    "    \"content-type\": f\"image/jpeg\",\n",
    "}\n",
    "# Convert image to jpeg before uploading\n",
    "image = Image.open(\"readme_assets/few_shot_arch_diagram.png\").convert(\"RGB\")\n",
    "buf = io.BytesIO()  # temporary buffer to save image\n",
    "image.save(buf, format=\"JPEG\") #convert image to jpeg to get smaller upload size \n",
    "\n",
    "# upload image\n",
    "response = requests.put(\n",
    "    asset_url,\n",
    "    data=buf.getvalue(),\n",
    "    headers=s3_headers,\n",
    "    timeout=300,\n",
    ")\n",
    "response.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e29011-7160-4aaa-be5d-851fe421238c",
   "metadata": {},
   "source": [
    "The image has been uploaded and it can now be referenced using the asset ID in any NIM API requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddd55a8-eb50-4e3b-952f-c18744e6aa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3) Send NVDINOv2 Request and reference uploaded image\n",
    "payload = {\"messages\": []} #payload can be just an empty \"messages\" field. Since the image is referenced in the header, no other informnation is needed in the payload. \n",
    "asset_list = f\"{asset_id}\"\n",
    "\n",
    "#Asset ID needs to be included in the header\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"NVCF-INPUT-ASSET-REFERENCES\": asset_id,\n",
    "    \"NVCF-FUNCTION-ASSET-IDS\": asset_id,\n",
    "    \"Authorization\": header_auth,\n",
    "}\n",
    "\n",
    "#Send NVDINOv2 request to generate the embedding\n",
    "response = requests.post(nvdinov2_url, headers=headers, json=payload)\n",
    "response = response.json()\n",
    "embedding = response[\"metadata\"][0][\"embedding\"] #get the embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5987d922-fa11-47d6-b682-69c70dfe9f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embedding))\n",
    "print(type(embedding[0]))\n",
    "#print(embedding) #uncomment to print entire embedding vector "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd465ba-5ec7-4e6f-9183-1c0f26b64d2a",
   "metadata": {},
   "source": [
    "In the response, we can get the embedding of our image. This is a 1536d vector that represents our image and can be used for downstream tasks such as classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a9ed42-0933-4df4-8ce2-f081f165aa68",
   "metadata": {},
   "source": [
    "## 3: Prepare Dataset\n",
    "\n",
    "To show how to use NVDINOv2 embeddings for few shot classification, we can use a [car classification dataset from HuggingFace](https://huggingface.co/datasets/tanganke/stanford_cars). The following cell will download the 6GB dataset. For each dataset an user elects to use, the user is responsible for checking if the dataset license is fit for the intended purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d885f66-b5bd-412e-8a44-5fabe4d54aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"tanganke/stanford_cars\") #6GB\n",
    "train_set = dataset[\"train\"]\n",
    "test_set = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113fe626-1fb5-4ddf-a86f-3ae4c3513b70",
   "metadata": {},
   "source": [
    "It takes 1 NIM credit to embed 1 image so we will generate a much smaller subset of the data to show how to build a few shot classification model. The subset will have three classes. Each class will have 10 test images and 5 train images. You can adjust the cell below to control the data in the subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e3a53d-f4e2-40c6-aa8d-6e7b53696460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 classes with 5 train images and 10 test images. \n",
    "num_classes = 3\n",
    "test_images_per_class = 10\n",
    "train_images_per_class = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ccdbc7-63ff-44ff-a61f-b4156140fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subset(dataset, classes, images_per_class):\n",
    "    \"\"\"Make subset with given list of classes and specified images per class\"\"\"\n",
    "    subset = []\n",
    "    label_counter = {}\n",
    "    for i, sample in enumerate(dataset):\n",
    "        label = sample[\"label\"]\n",
    "        if label not in classes:\n",
    "            continue \n",
    "            \n",
    "        label_count = label_counter.get(label, 0)\n",
    "    \n",
    "        if label_count < images_per_class:\n",
    "            subset.append(sample)\n",
    "            label_counter[label] = label_counter.get(label, 0) + 1\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a67464-87d7-4457-863f-c9c6a68668ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make subsets to train and test on\n",
    "train_subset = make_subset(dataset[\"train\"], range(num_classes), train_images_per_class)\n",
    "test_subset = make_subset(dataset[\"train\"], range(num_classes), test_images_per_class)\n",
    "print(len(train_subset))\n",
    "print(len(test_subset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c499473c-5477-494e-9812-6fed0cdb4fdc",
   "metadata": {},
   "source": [
    "To make it easier to generate the embeddings, a NVDINOv2 wrapper classes has been implemented in the nvdinov2.py script in the same directory as this notebook. This will handle the image upload and embedding calls. It can be passed a list of image paths or PIL images. It will return a list of embeddings for each image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f3b6d-b03f-4c6d-87f1-0ace87ca509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvdinov2 import NVDINOv2\n",
    "def add_embeddings(api_key, dataset):\n",
    "    nvdinov2 = NVDINOv2(api_key)\n",
    "    pil_images = [x[\"image\"] for x in dataset] #get PIL images from dataset\n",
    "    embeddings = nvdinov2(pil_images) #pass list of images to nvdinov2\n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i][\"embedding\"] = embeddings[i]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f96a309-f4e0-457b-85e5-00aef2436642",
   "metadata": {},
   "source": [
    "NVCLIP is another embedding model availalbe as a NIM that can also be used for few shot classification. If you want to see how it compares to NVDINOv2, then uncomment the cell below to replace the embeddings from NVDINOv2 with embeddings from NVCLIP and run the rest of the notebook. If you want to use NVCLIP, then you will also need change the embedding dimension in section 3.2 from 1536 to 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ecf3a9-9218-4686-ad84-ff7373d01f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nvclip import NVCLIP\n",
    "# def add_embeddings(api_key, dataset):\n",
    "#     nvclip = NVCLIP(api_key)\n",
    "#     pil_images = [x[\"image\"] for x in dataset]\n",
    "#     print(len(pil_images))\n",
    "#     embeddings = nvclip(pil_images)\n",
    "#     print(embeddings)\n",
    "#     print(len(embeddings))\n",
    "#     for i in range(len(dataset)):\n",
    "#         dataset[i][\"embedding\"] = embeddings[i]\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61f1972-253d-431f-ac43-4c7b0c2ad13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the embeddings to the test and train dataset \n",
    "train_subset = add_embeddings(api_key, train_subset)\n",
    "test_subset = add_embeddings(api_key, test_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1686e61-278d-42b4-8781-138135caf513",
   "metadata": {},
   "source": [
    "The following cell will plot the test data in 2 dimensions so it can be visually inspected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9697aab-ac16-434b-9fa2-970fe842ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Bokeh output in Jupyter Notebooks\n",
    "output_notebook()\n",
    "vectors = np.array([x[\"embedding\"] for x in test_subset])\n",
    "class_labels =  np.array([x[\"label\"] for x in test_subset])\n",
    "\n",
    "\n",
    "#Use TSNE to project the embeddings to 2D\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=5,\n",
    "    learning_rate=200,\n",
    "    early_exaggeration=5,\n",
    "    n_iter=2000,\n",
    "    random_state=42,\n",
    "    metric=\"cosine\",\n",
    ")\n",
    "embedding_2d = tsne.fit_transform(vectors)\n",
    "\n",
    "p = figure(\n",
    "    title=\"Embedding Visualization\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,save\",\n",
    ")\n",
    "colors = itertools.cycle(palette)\n",
    "\n",
    "#Plot each class \n",
    "for n in np.unique(class_labels):\n",
    "    indices = np.where(class_labels == n)\n",
    "    n_vectors = embedding_2d[indices]\n",
    "    x = n_vectors[:, 0]\n",
    "    y = n_vectors[:, 1]\n",
    "\n",
    "    source = ColumnDataSource(dict(x=x, y=y))\n",
    "\n",
    "    p.scatter(\"x\", \"y\", source=source, size=8, color=next(colors), legend_label=str(n))\n",
    "\n",
    "# Layout\n",
    "layout = column(p)\n",
    "# Show plot in notebook\n",
    "show(layout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ab9ba6-fd2e-4bc3-acea-57d203c8f626",
   "metadata": {},
   "source": [
    "Now that the image embeddings have been generated, few shot classifcation can be implemented using models from SKLearn or with a KNN algorithm and a Milvus vector database. Both methods will be explored in the following sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cd8329-5816-4182-bc06-798bfa64e98c",
   "metadata": {},
   "source": [
    "# 4: Few Shot Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2f85a0-2d9d-4bb7-aa80-0d624f935792",
   "metadata": {},
   "source": [
    "At this stage, each image in our test and train datasets has an associated image embedding generated by NVDINOv2 or NVCLIP. These image embeddings are compressed versions of the image that contain the most important information needed to understand the contents of the image. A property of these embeddings, is that images that are similar to each other will be close together in the embedding space. In the plot from Part 2, images of the same class should appear near each other and form clusters. Because these embeddings (also known as feature vectors) contain enough information to differentiate images of different classes, they can be used as input to simple classification models such as Logisitic Regression or a KNN algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7eece0-3712-48c2-9e95-4c08e2fb2b28",
   "metadata": {},
   "source": [
    "## 4.1 SKLearn \n",
    "\n",
    "SKLearn provides several classification models that can be used with the image embeddings. Each classification model requires a set of features and labels to train. In this case the embeddings are the features and the class ID is the label. \n",
    "\n",
    "Now we can combine the embeddings with a light weight classification head from SKLearn such as [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) and [K Nearest Neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). A benefit of using a powerful embedding model is it lets us achieve high accuracy with very few images. This drastically reduces the amount of computation needed to produce the classification model and can be done without a GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2628144e-591a-4d41-81d1-ec29494b483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Split into labels and features \n",
    "x_test = [x[\"embedding\"] for x in test_subset]\n",
    "y_test = [x[\"label\"] for x in test_subset]\n",
    "\n",
    "x_train = [x[\"embedding\"] for x in train_subset]\n",
    "y_train = [x[\"label\"] for x in train_subset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea7f70-2141-40b2-b905-b5280a58314a",
   "metadata": {},
   "source": [
    "The following two cells with train and test a logistic regression and KNN classifaction models on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e70db7c-5e6c-4b3f-a293-fd0906e8d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit and test logistic regression head \n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)\n",
    "y_predict = model.predict(x_test)\n",
    "report = classification_report(y_test, y_predict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61515069-07a6-424b-b013-d78b6f82f8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit and test knn classification head \n",
    "model = KNeighborsClassifier(n_neighbors=3, weights=\"distance\")\n",
    "model.fit(x_train, y_train)\n",
    "y_predict = model.predict(x_test)\n",
    "report = classification_report(y_test, y_predict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce78f52d-b041-4f60-a2bf-fe753471e4d0",
   "metadata": {},
   "source": [
    "From the classification report, you can see that the models can get 90% accuracy with only 5 training images per class and it required very little compute resources to generate the few shot classifiation models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c4a769-c52a-4058-be0d-e865c458b41d",
   "metadata": {},
   "source": [
    "## 4.2 Milvus Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a236920-af17-42bc-99b0-056948d51ddb",
   "metadata": {},
   "source": [
    "One advantage of using a K-Nearest Neighbors (KNN) classification algorithm is that it can be efficiently scaled and implemented with a vector database. A vector database enables the quick insertion of new image embeddings and allows for fast similarity searches. We've already discussed that images belonging to the same class tend to be close together in the embedding space. By storing the image embeddings from our training set in the vector database, we can classify a new image by searching for the most similar embeddings (nearest neighbors) and their associated labels. The most common label among these nearest neighbors is then predicted as the label for the new image. This is the basic principle behind how the [K-Nearest Neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) algorithm works.\n",
    "\n",
    "Using Milvus, a KNN Classifier is implemented with 'add' and 'predict' methods. This allows new examples and classes to be inserted in the database as needed and predictions to always take into account the latest samples in the database. \n",
    "\n",
    "To learn more about Milvus visit their [documentation page](https://milvus.io/docs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99288d41-da7b-4a2d-b45c-ada0b1d38c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "class KNeighborsClassifierMilvus:\n",
    "    def __init__(self, database=\"milvus_demo.db\", collection=\"knn\", rm=True, embedding_d=1536):\n",
    "        \n",
    "        self.database = database\n",
    "        self.collection = collection\n",
    "        self.id_tracker = 0 #track IDs to insert new data\n",
    "\n",
    "        #Delete local database if it exists\n",
    "        if rm:\n",
    "            Path.unlink(self.database, missing_ok=True)\n",
    "\n",
    "        #connect to database\n",
    "        self.client = MilvusClient(self.database)\n",
    "       \n",
    "        #setup collection \n",
    "        if not self.client.has_collection(collection_name=self.collection):\n",
    "            #create collection in database. This will associate a vector with the metadata\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection,\n",
    "                dimension=embedding_d, #1536 for NVDINOv2, 1024 for NVCLIP \n",
    "                metric_type=\"L2\"\n",
    "                )\n",
    "        \n",
    "    def add(self, x, y):\n",
    "        \"\"\"Add labelled embeddings to classifier\"\"\"\n",
    "        milvus_samples = []\n",
    "        for i, vector in enumerate(x):\n",
    "            sample = {\"id\":self.id_tracker, \"vector\":vector, \"label\":y[i]}\n",
    "            self.id_tracker += 1\n",
    "            milvus_samples.append(sample)\n",
    "        self.client.insert(collection_name=self.collection, data=milvus_samples)\n",
    "    def predict(self, x, n_neighbors=1):\n",
    "        \"\"\"pass in 2d list of vectors\"\"\"\n",
    "        labels = []\n",
    "        results = self.client.search(collection_name=self.collection, data=x, limit=n_neighbors, output_fields=[\"label\"])\n",
    "        for result in results:\n",
    "            neighbor_labels = [x[\"entity\"][\"label\"] for x in result]\n",
    "            label_counter = Counter(neighbor_labels)\n",
    "            label = label_counter.most_common()[0][0] #get most common label from neighbors \n",
    "            labels.append(label)\n",
    "        return labels \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f222c7-5452-4346-a768-5024791d8590",
   "metadata": {},
   "source": [
    "Now we can run this and compare the results with the SKLearn models. The accuracy should be similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dbe496-f0f8-4dc4-abdd-f81b0f4f2fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifierMilvus(embedding_d=1536) #pass embedding_d=1024 if using NVCLIP embeddings\n",
    "model.add(x_train, y_train)\n",
    "y_predict = model.predict(x_test, n_neighbors=2)\n",
    "report = classification_report(y_test, y_predict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a4d10b",
   "metadata": {},
   "source": [
    "Summary of Optimizations and Added Functionality\n",
    "Error Handling: Incorporated robust error handling and logging for all major operations (API calls, image uploads, inferences).\n",
    "Modularization: Refactored the code into clear and reusable functions for better readability and maintainability.\n",
    "Gradio UI: Implemented a dynamic Gradio interface for real-time image classification, allowing users to upload images and receive predictions.\n",
    "Comparison: Introduced a baseline comparison using KNeighborsClassifier from SKLearn to evaluate the performance of NVDINOv2.\n",
    "You can extend this notebook further by integrating additional models, adding support for batch processing, or expanding the Gradio UI to include more interactive options like model selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
